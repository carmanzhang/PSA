1. calculate lexical overlap (or other measurement can quantify text similarity at lexical level) to demonstrate the advantage of our data generation method. The result should place at Analysis/Ablation section.
    Competing methods:
        PubMed official ranking result
        PubMed citation BM25 ranking result,
        Only Original MeSH based method
        Only Original Citation based method
        Original MeSH + Original Citation based method
        Only Enhanced MeSH based method
        Only Enhanced Citation based method
        Enhanced MeSH + Enhanced Citation based method (Our)

2. retrieval more "ground truth" result for further evaluation
    speed up searching using fast tools

3. make two kinds of evaluation result.
    online/offline evaluation.
    offline evaluation means predict the order of fixed-length ranking "ground truth" only on evaluation dataset.
    online evaluation means retrieving the most similar results using encoded embedding + similar search, then evaluate the ranking result against the ground truth ranking result.
    Note that, in online evaluation, the  similar articles in official PubMed will be introduce for comparison.

    For offline evaluation, the test set should be a fixed-length sequence of PMID which is ordered by the similarity with the anchor PMID. The train/dev set can be different according to loss that model used.
    The online evaluation goes further, its dataset is the same as the offline evaluation; however, the goal of final evaluation is not to evaluate on the re-ranking result on fixed-size ranking result.
    Instead, it will evaluate that what the extent of the embedding-based similar search will be using fine-tuned models (available from offline step), the performance should be measured against the test set of ground truth ranking result.
    Note that, to reach this aim, we must drive all the embedding for the entire PubMed.


Evaluation dataset:
    1. https://dmice.ohsu.edu/trec-gen/data/2005/  http://skynet.ohsu.edu/trec-gen/data/2005/genomics.qrels.large.txt
    2. http://trec-cds.appspot.com/qrels2014.txt

how to organize two papers?
how to organize the baselines?

Baselines:
    see my MS Word document

paper 1: Data innovation, focus on how to train, extensive evaluation of sentence-transformers
    - training dataset building method
    - at least tow benchmark dataset and
    b. analysis
        - MeSH/citation enhancement abalation test, consider data building methods, distance or label, this is inportant as it will influence which loss we use
            current information can be uses: MeSH Citation Citation Couping, and Single Sentence Citation
        - document length. discussion the influence of articles length to our methods and BM25, as sentence transormer usually bad at documnet-level, while BM25 is a well-designed retrival algo for document search.
        - tune hyper parameters of BM25, as it have great impact on the performance

paper 2: Model innovation, focus on best performance, fancy model, embedding generation, and the usefullness of the embedding and our methods. and genelization on other tasks, such as STS, NLI.

